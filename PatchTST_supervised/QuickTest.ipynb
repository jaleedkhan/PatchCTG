{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52316ecc-9a69-4bb3-9a2a-3f73c185f81b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(activation='gelu', affine=0, batch_size=8, c_out=7, checkpoints='./checkpoints/', d_ff=64, d_layers=1, d_model=32, data='custom', data_path='weather.csv', dec_in=7, decomposition=0, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.1, e_layers=1, embed='timeF', embed_type=0, enc_in=21, factor=1, fc_dropout=0.1, features='M', freq='h', gpu=0, head_dropout=0.0, individual=0, is_training=1, itr=1, kernel_size=25, label_len=48, learning_rate=0.0001, loss='mse', lradj='type3', model='PatchTST', model_id='weather_quick_test_48_24', moving_avg=25, n_heads=4, num_workers=10, output_attention=False, padding_patch='end', patch_len=4, patience=1, pct_start=0.3, pred_len=24, random_seed=2021, revin=1, root_path='./dataset/weather/', seq_len=48, stride=2, subtract_last=0, target='OT', test_flop=False, train_epochs=1, use_amp=False, use_gpu=False, use_multi_gpu=False)\n",
      "Use CPU\n",
      ">>>>>>>start training : weather_quick_test_48_24_PatchTST_custom_ftM_sl48_ll48_pl24_dm32_nh4_el1_dl1_df64_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 36816\n",
      "val 5247\n",
      "test 10516\n",
      "\titers: 100, epoch: 1 | loss: 0.3585793\n",
      "\tspeed: 0.0242s/iter; left time: 109.0134s\n",
      "\titers: 200, epoch: 1 | loss: 0.3936895\n",
      "\tspeed: 0.0209s/iter; left time: 92.0685s\n",
      "\titers: 300, epoch: 1 | loss: 0.5610700\n",
      "\tspeed: 0.0193s/iter; left time: 83.1148s\n",
      "\titers: 400, epoch: 1 | loss: 0.6072474\n",
      "\tspeed: 0.0169s/iter; left time: 70.9993s\n",
      "\titers: 500, epoch: 1 | loss: 0.7040648\n",
      "\tspeed: 0.0171s/iter; left time: 70.1027s\n",
      "\titers: 600, epoch: 1 | loss: 0.7261629\n",
      "\tspeed: 0.0171s/iter; left time: 68.3039s\n",
      "\titers: 700, epoch: 1 | loss: 0.4695016\n",
      "\tspeed: 0.0179s/iter; left time: 69.7207s\n",
      "\titers: 800, epoch: 1 | loss: 0.2402541\n",
      "\tspeed: 0.0167s/iter; left time: 63.5799s\n",
      "\titers: 900, epoch: 1 | loss: 2.3032768\n",
      "\tspeed: 0.0166s/iter; left time: 61.5190s\n",
      "\titers: 1000, epoch: 1 | loss: 0.5053053\n",
      "\tspeed: 0.0162s/iter; left time: 58.4117s\n",
      "\titers: 1100, epoch: 1 | loss: 0.2723399\n",
      "\tspeed: 0.0159s/iter; left time: 55.6319s\n",
      "\titers: 1200, epoch: 1 | loss: 0.3902872\n",
      "\tspeed: 0.0163s/iter; left time: 55.3920s\n",
      "\titers: 1300, epoch: 1 | loss: 0.3328434\n",
      "\tspeed: 0.0166s/iter; left time: 54.6833s\n",
      "\titers: 1400, epoch: 1 | loss: 0.4065500\n",
      "\tspeed: 0.0165s/iter; left time: 52.9063s\n",
      "\titers: 1500, epoch: 1 | loss: 0.2630463\n",
      "\tspeed: 0.0161s/iter; left time: 49.9852s\n",
      "\titers: 1600, epoch: 1 | loss: 0.2948745\n",
      "\tspeed: 0.0165s/iter; left time: 49.4490s\n",
      "\titers: 1700, epoch: 1 | loss: 0.5089489\n",
      "\tspeed: 0.0156s/iter; left time: 45.2445s\n",
      "\titers: 1800, epoch: 1 | loss: 0.3682336\n",
      "\tspeed: 0.0166s/iter; left time: 46.4569s\n",
      "\titers: 1900, epoch: 1 | loss: 0.2554444\n",
      "\tspeed: 0.0165s/iter; left time: 44.4780s\n",
      "\titers: 2000, epoch: 1 | loss: 0.7543341\n",
      "\tspeed: 0.0162s/iter; left time: 42.2596s\n",
      "\titers: 2100, epoch: 1 | loss: 0.6074027\n",
      "\tspeed: 0.0165s/iter; left time: 41.2057s\n",
      "\titers: 2200, epoch: 1 | loss: 0.5479980\n",
      "\tspeed: 0.0162s/iter; left time: 39.0364s\n",
      "\titers: 2300, epoch: 1 | loss: 0.4629761\n",
      "\tspeed: 0.0158s/iter; left time: 36.3166s\n",
      "\titers: 2400, epoch: 1 | loss: 0.5058280\n",
      "\tspeed: 0.0160s/iter; left time: 35.3414s\n",
      "\titers: 2500, epoch: 1 | loss: 0.1757609\n",
      "\tspeed: 0.0165s/iter; left time: 34.6498s\n",
      "\titers: 2600, epoch: 1 | loss: 0.3275348\n",
      "\tspeed: 0.0169s/iter; left time: 33.7849s\n",
      "\titers: 2700, epoch: 1 | loss: 0.3877724\n",
      "\tspeed: 0.0160s/iter; left time: 30.5297s\n",
      "\titers: 2800, epoch: 1 | loss: 0.4802722\n",
      "\tspeed: 0.0162s/iter; left time: 29.2402s\n",
      "\titers: 2900, epoch: 1 | loss: 0.4118945\n",
      "\tspeed: 0.0169s/iter; left time: 28.8178s\n",
      "\titers: 3000, epoch: 1 | loss: 0.2735800\n",
      "\tspeed: 0.0206s/iter; left time: 33.0493s\n",
      "\titers: 3100, epoch: 1 | loss: 0.2792638\n",
      "\tspeed: 0.0201s/iter; left time: 30.1542s\n",
      "\titers: 3200, epoch: 1 | loss: 0.2165971\n",
      "\tspeed: 0.0213s/iter; left time: 29.8299s\n",
      "\titers: 3300, epoch: 1 | loss: 0.3264163\n",
      "\tspeed: 0.0177s/iter; left time: 23.0748s\n",
      "\titers: 3400, epoch: 1 | loss: 0.4700756\n",
      "\tspeed: 0.0160s/iter; left time: 19.2948s\n",
      "\titers: 3500, epoch: 1 | loss: 0.2755162\n",
      "\tspeed: 0.0165s/iter; left time: 18.2352s\n",
      "\titers: 3600, epoch: 1 | loss: 0.3465722\n",
      "\tspeed: 0.0193s/iter; left time: 19.3650s\n",
      "\titers: 3700, epoch: 1 | loss: 0.3523376\n",
      "\tspeed: 0.0207s/iter; left time: 18.7315s\n",
      "\titers: 3800, epoch: 1 | loss: 0.4202603\n",
      "\tspeed: 0.0208s/iter; left time: 16.6785s\n",
      "\titers: 3900, epoch: 1 | loss: 0.2384946\n",
      "\tspeed: 0.0203s/iter; left time: 14.2987s\n",
      "\titers: 4000, epoch: 1 | loss: 2.6661975\n",
      "\tspeed: 0.0196s/iter; left time: 11.7974s\n",
      "\titers: 4100, epoch: 1 | loss: 0.2858635\n",
      "\tspeed: 0.0199s/iter; left time: 10.0230s\n",
      "\titers: 4200, epoch: 1 | loss: 0.3486850\n",
      "\tspeed: 0.0193s/iter; left time: 7.7968s\n",
      "\titers: 4300, epoch: 1 | loss: 0.2084623\n",
      "\tspeed: 0.0199s/iter; left time: 6.0297s\n",
      "\titers: 4400, epoch: 1 | loss: 0.3170950\n",
      "\tspeed: 0.0193s/iter; left time: 3.9098s\n",
      "\titers: 4500, epoch: 1 | loss: 0.4112671\n",
      "\tspeed: 0.0191s/iter; left time: 1.9688s\n",
      "\titers: 4600, epoch: 1 | loss: 0.2262856\n",
      "\tspeed: 0.0190s/iter; left time: 0.0571s\n",
      "Epoch: 1 cost time: 82.27851057052612\n",
      "Epoch: 1, Steps: 4602 | Train Loss: 0.5897830 Vali Loss: 0.4319603 Test Loss: 0.1364214\n",
      "Validation loss decreased (inf --> 0.431960).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : weather_quick_test_48_24_PatchTST_custom_ftM_sl48_ll48_pl24_dm32_nh4_el1_dl1_df64_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 10516\n",
      "mse:0.13642147183418274, mae:0.16481135785579681, rse:0.487060010433197\n"
     ]
    }
   ],
   "source": [
    "!sh scripts/PatchTST/weather_quick.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa8f34dd-2dc6-4f6f-8df8-730e830ee4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the loaded state_dict:\n",
      "odict_keys(['model.backbone.W_pos', 'model.backbone.W_P.weight', 'model.backbone.W_P.bias', 'model.backbone.encoder.layers.0.self_attn.W_Q.weight', 'model.backbone.encoder.layers.0.self_attn.W_Q.bias', 'model.backbone.encoder.layers.0.self_attn.W_K.weight', 'model.backbone.encoder.layers.0.self_attn.W_K.bias', 'model.backbone.encoder.layers.0.self_attn.W_V.weight', 'model.backbone.encoder.layers.0.self_attn.W_V.bias', 'model.backbone.encoder.layers.0.self_attn.sdp_attn.scale', 'model.backbone.encoder.layers.0.self_attn.to_out.0.weight', 'model.backbone.encoder.layers.0.self_attn.to_out.0.bias', 'model.backbone.encoder.layers.0.norm_attn.1.weight', 'model.backbone.encoder.layers.0.norm_attn.1.bias', 'model.backbone.encoder.layers.0.norm_attn.1.running_mean', 'model.backbone.encoder.layers.0.norm_attn.1.running_var', 'model.backbone.encoder.layers.0.norm_attn.1.num_batches_tracked', 'model.backbone.encoder.layers.0.ff.0.weight', 'model.backbone.encoder.layers.0.ff.0.bias', 'model.backbone.encoder.layers.0.ff.3.weight', 'model.backbone.encoder.layers.0.ff.3.bias', 'model.backbone.encoder.layers.0.norm_ffn.1.weight', 'model.backbone.encoder.layers.0.norm_ffn.1.bias', 'model.backbone.encoder.layers.0.norm_ffn.1.running_mean', 'model.backbone.encoder.layers.0.norm_ffn.1.running_var', 'model.backbone.encoder.layers.0.norm_ffn.1.num_batches_tracked', 'model.head.linear.weight', 'model.head.linear.bias'])\n",
      "model.backbone.W_pos: torch.Size([24, 32])\n",
      "model.backbone.W_P.weight: torch.Size([32, 4])\n",
      "model.backbone.W_P.bias: torch.Size([32])\n",
      "model.backbone.encoder.layers.0.self_attn.W_Q.weight: torch.Size([32, 32])\n",
      "model.backbone.encoder.layers.0.self_attn.W_Q.bias: torch.Size([32])\n",
      "model.backbone.encoder.layers.0.self_attn.W_K.weight: torch.Size([32, 32])\n",
      "model.backbone.encoder.layers.0.self_attn.W_K.bias: torch.Size([32])\n",
      "model.backbone.encoder.layers.0.self_attn.W_V.weight: torch.Size([32, 32])\n",
      "model.backbone.encoder.layers.0.self_attn.W_V.bias: torch.Size([32])\n",
      "model.backbone.encoder.layers.0.self_attn.sdp_attn.scale: torch.Size([])\n",
      "model.backbone.encoder.layers.0.self_attn.to_out.0.weight: torch.Size([32, 32])\n",
      "model.backbone.encoder.layers.0.self_attn.to_out.0.bias: torch.Size([32])\n",
      "model.backbone.encoder.layers.0.norm_attn.1.weight: torch.Size([32])\n",
      "model.backbone.encoder.layers.0.norm_attn.1.bias: torch.Size([32])\n",
      "model.backbone.encoder.layers.0.norm_attn.1.running_mean: torch.Size([32])\n",
      "model.backbone.encoder.layers.0.norm_attn.1.running_var: torch.Size([32])\n",
      "model.backbone.encoder.layers.0.norm_attn.1.num_batches_tracked: torch.Size([])\n",
      "model.backbone.encoder.layers.0.ff.0.weight: torch.Size([64, 32])\n",
      "model.backbone.encoder.layers.0.ff.0.bias: torch.Size([64])\n",
      "model.backbone.encoder.layers.0.ff.3.weight: torch.Size([32, 64])\n",
      "model.backbone.encoder.layers.0.ff.3.bias: torch.Size([32])\n",
      "model.backbone.encoder.layers.0.norm_ffn.1.weight: torch.Size([32])\n",
      "model.backbone.encoder.layers.0.norm_ffn.1.bias: torch.Size([32])\n",
      "model.backbone.encoder.layers.0.norm_ffn.1.running_mean: torch.Size([32])\n",
      "model.backbone.encoder.layers.0.norm_ffn.1.running_var: torch.Size([32])\n",
      "model.backbone.encoder.layers.0.norm_ffn.1.num_batches_tracked: torch.Size([])\n",
      "model.head.linear.weight: torch.Size([24, 768])\n",
      "model.head.linear.bias: torch.Size([24])\n",
      "RuntimeError during model state_dict loading: Error(s) in loading state_dict for Model:\n",
      "\tsize mismatch for model.backbone.W_pos: copying a param with shape torch.Size([24, 32]) from checkpoint, the shape in current model is torch.Size([13, 32]).\n",
      "\tsize mismatch for model.head.linear.weight: copying a param with shape torch.Size([24, 768]) from checkpoint, the shape in current model is torch.Size([24, 416]).\n",
      "Input sequence:\n",
      "                   date  p (mbar)  T (degC)  Tpot (K)  Tdew (degC)    rh (%)  \\\n",
      "0   2020-10-20 03:30:00  0.721556 -0.222681 -0.284147    -0.138359  0.011731   \n",
      "1   2020-10-20 03:40:00  0.715762 -0.209725 -0.270252    -0.101233  0.048520   \n",
      "2   2020-10-20 03:50:00  0.706493 -0.203247 -0.262673    -0.019245  0.174353   \n",
      "3   2020-10-20 04:00:00  0.698383 -0.186404 -0.246252    -0.006870  0.162622   \n",
      "4   2020-10-20 04:10:00  0.691431 -0.155309 -0.214673    -0.053278  0.024527   \n",
      "5   2020-10-20 04:20:00  0.678686 -0.122919 -0.181831    -0.030074  0.004266   \n",
      "6   2020-10-20 04:30:00  0.668258 -0.093120 -0.152779    -0.087310 -0.141827   \n",
      "7   2020-10-20 04:40:00  0.657830 -0.043888 -0.103516    -0.113608 -0.267126   \n",
      "8   2020-10-20 04:50:00  0.654354 -0.030932 -0.090884    -0.407525 -0.705939   \n",
      "9   2020-10-20 05:00:00  0.648561 -0.043888 -0.102252    -0.430729 -0.717669   \n",
      "10  2020-10-20 05:10:00  0.645085 -0.058139 -0.116147    -0.461668 -0.736863   \n",
      "11  2020-10-20 05:20:00  0.641609 -0.050366 -0.108568    -0.410619 -0.681945   \n",
      "12  2020-10-20 05:30:00  0.639291 -0.027045 -0.085831    -0.422995 -0.731532   \n",
      "13  2020-10-20 05:40:00  0.632339  0.004050 -0.054253    -0.432276 -0.789649   \n",
      "14  2020-10-20 05:50:00  0.631181  0.005345 -0.052989    -0.388962 -0.734197   \n",
      "15  2020-10-20 06:00:00  0.626546 -0.010202 -0.068147    -0.410619 -0.740063   \n",
      "16  2020-10-20 06:10:00  0.620753 -0.010202 -0.066884    -0.430729 -0.767255   \n",
      "17  2020-10-20 06:20:00  0.609166 -0.007611 -0.063095    -0.474043 -0.825906   \n",
      "18  2020-10-20 06:30:00  0.604532  0.018301 -0.037832    -0.484872 -0.876558   \n",
      "19  2020-10-20 06:40:00  0.608008  0.005345 -0.050463    -0.506529 -0.885622   \n",
      "20  2020-10-20 06:50:00  0.611484 -0.024454 -0.080779    -0.546749 -0.895753   \n",
      "21  2020-10-20 07:00:00  0.602214 -0.030932 -0.085831    -0.543655 -0.882957   \n",
      "22  2020-10-20 07:10:00  0.592945 -0.023158 -0.076989    -0.511170 -0.851499   \n",
      "23  2020-10-20 07:20:00  0.587152 -0.028340 -0.082042    -0.467856 -0.788583   \n",
      "24  2020-10-20 07:30:00  0.588311 -0.052957 -0.106042    -0.351836 -0.597702   \n",
      "25  2020-10-20 07:40:00  0.587152 -0.072391 -0.124989    -0.308522 -0.505994   \n",
      "26  2020-10-20 07:50:00  0.584835 -0.085347 -0.137621    -0.297693 -0.469204   \n",
      "27  2020-10-20 08:00:00  0.569772 -0.097007 -0.147726    -0.320897 -0.485199   \n",
      "28  2020-10-20 08:10:00  0.584835 -0.106076 -0.157831    -0.300787 -0.442011   \n",
      "29  2020-10-20 08:20:00  0.596421 -0.116441 -0.169200    -0.262114 -0.368431   \n",
      "30  2020-10-20 08:30:00  0.604532 -0.143649 -0.195726    -0.302334 -0.382828   \n",
      "31  2020-10-20 08:40:00  0.604532 -0.157901 -0.209621    -0.331726 -0.401489   \n",
      "32  2020-10-20 08:50:00  0.609166 -0.169561 -0.222252    -0.333272 -0.384960   \n",
      "33  2020-10-20 09:00:00  0.616118 -0.190291 -0.242463    -0.303881 -0.306582   \n",
      "34  2020-10-20 09:10:00  0.620753 -0.211020 -0.262673    -0.280677 -0.235668   \n",
      "35  2020-10-20 09:20:00  0.617277 -0.235637 -0.286673    -0.255926 -0.153024   \n",
      "36  2020-10-20 09:30:00  0.610325 -0.262844 -0.313200    -0.254379 -0.101838   \n",
      "37  2020-10-20 09:40:00  0.630022 -0.301712 -0.352357    -0.221893  0.022928   \n",
      "38  2020-10-20 09:50:00  0.645085 -0.328920 -0.380147    -0.294599 -0.045320   \n",
      "39  2020-10-20 10:00:00  0.638133 -0.371675 -0.421831    -0.330179 -0.025592   \n",
      "40  2020-10-20 10:10:00  0.630022 -0.449411 -0.496357    -0.243550  0.275658   \n",
      "41  2020-10-20 10:20:00  0.619594 -0.501235 -0.546884    -0.289958  0.296452   \n",
      "42  2020-10-20 10:30:00  0.598738 -0.511600 -0.554462    -0.232722  0.422284   \n",
      "43  2020-10-20 10:40:00  0.580200 -0.542694 -0.583515    -0.255926  0.445744   \n",
      "44  2020-10-20 10:50:00  0.568614 -0.558241 -0.598673    -0.268301  0.456408   \n",
      "45  2020-10-20 11:00:00  0.570931 -0.554355 -0.594883    -0.302334  0.386561   \n",
      "46  2020-10-20 11:10:00  0.558186 -0.577675 -0.615094    -0.296146  0.445744   \n",
      "47  2020-10-20 11:20:00  0.554710 -0.623022 -0.659304    -0.285318  0.565178   \n",
      "\n",
      "    VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  ...  wv (m/s)  \\\n",
      "0      -0.391404     -0.310371     -0.306539  -0.323485  ...  0.002545   \n",
      "1      -0.381538     -0.278525     -0.316704  -0.291340  ... -0.001102   \n",
      "2      -0.375373     -0.203460     -0.364139  -0.219909  ... -0.001294   \n",
      "3      -0.361807     -0.192086     -0.353975  -0.209194  ... -0.014154   \n",
      "4      -0.337144     -0.235306     -0.286209  -0.252053  ... -0.000334   \n",
      "5      -0.310014     -0.214833     -0.265880  -0.227052  ... -0.000718   \n",
      "6      -0.285350     -0.267151     -0.193032  -0.280626  ... -0.007820   \n",
      "7      -0.243422     -0.289898     -0.118490  -0.302055  ... -0.004557   \n",
      "8      -0.232323     -0.535566      0.079724  -0.544923  ...  0.016365   \n",
      "9      -0.243422     -0.553764      0.078030  -0.562780  ...  0.020203   \n",
      "10     -0.255754     -0.576511      0.078030  -0.584210  ...  0.028073   \n",
      "11     -0.248354     -0.537841      0.059394  -0.548494  ...  0.015981   \n",
      "12     -0.228623     -0.546940      0.093277  -0.555637  ...  0.010798   \n",
      "13     -0.201493     -0.553764      0.135631  -0.562780  ...  0.002545   \n",
      "14     -0.200260     -0.519643      0.111913  -0.530636  ...  0.020011   \n",
      "15     -0.213825     -0.537841      0.106830  -0.544923  ...  0.018668   \n",
      "16     -0.213825     -0.553764      0.118689  -0.562780  ...  0.005808   \n",
      "17     -0.211359     -0.585609      0.145795  -0.594925  ...  0.009263   \n",
      "18     -0.187928     -0.594708      0.183066  -0.602068  ...  0.000433   \n",
      "19     -0.200260     -0.610631      0.179678  -0.619926  ...  0.021547   \n",
      "20     -0.226157     -0.640202      0.166125  -0.648498  ...  0.026729   \n",
      "21     -0.232323     -0.637927      0.157654  -0.644927  ...  0.006576   \n",
      "22     -0.224924     -0.612906      0.147489  -0.619926  ...  0.002929   \n",
      "23     -0.229857     -0.581060      0.116995  -0.587782  ...  0.010990   \n",
      "24     -0.250821     -0.490072      0.020429  -0.498492  ...  0.028649   \n",
      "25     -0.266852     -0.455952     -0.028701  -0.466348  ...  0.018860   \n",
      "26     -0.277951     -0.446853     -0.050724  -0.455633  ...  0.016173   \n",
      "27     -0.287816     -0.465050     -0.049030  -0.473491  ...  0.024810   \n",
      "28     -0.296449     -0.449127     -0.072748  -0.459205  ... -0.001102   \n",
      "29     -0.305081     -0.417282     -0.108325  -0.427060  ... -0.003213   \n",
      "30     -0.327278     -0.451402     -0.113407  -0.459205  ...  0.003505   \n",
      "31     -0.339610     -0.474149     -0.113407  -0.484206  ...  0.011950   \n",
      "32     -0.348242     -0.476424     -0.125266  -0.484206  ...  0.005040   \n",
      "33     -0.365507     -0.451402     -0.165926  -0.462776  ... -0.001870   \n",
      "34     -0.381538     -0.433205     -0.203197  -0.441347  ... -0.001486   \n",
      "35     -0.401269     -0.412732     -0.245550  -0.423489  ... -0.009356   \n",
      "36     -0.423467     -0.410458     -0.276044  -0.419917  ... -0.012235   \n",
      "37     -0.453063     -0.383161     -0.337033  -0.394916  ... -0.004365   \n",
      "38     -0.474027     -0.444578     -0.321786  -0.455633  ...  0.002161   \n",
      "39     -0.506090     -0.474149     -0.342116  -0.484206  ...  0.001393   \n",
      "40     -0.562817     -0.401359     -0.474258  -0.412774  ... -0.014730   \n",
      "41     -0.599812     -0.440029     -0.496282  -0.452061  ... -0.010699   \n",
      "42     -0.607211     -0.392260     -0.542024  -0.402059  ... -0.009739   \n",
      "43     -0.628175     -0.412732     -0.557271  -0.423489  ... -0.012427   \n",
      "44     -0.639274     -0.421831     -0.564047  -0.434204  ... -0.008396   \n",
      "45     -0.636808     -0.449127     -0.540329  -0.459205  ... -0.009356   \n",
      "46     -0.652839     -0.444578     -0.564047  -0.455633  ... -0.015306   \n",
      "47     -0.682436     -0.435479     -0.613177  -0.444918  ... -0.009931   \n",
      "\n",
      "    max. wv (m/s)  wd (deg)  rain (mm)  raining (s)  SWDR (W/m�)  \\\n",
      "0       -0.266122  0.206001  -0.093506     -0.22105    -0.056462   \n",
      "1       -0.296939  0.049615  -0.093506     -0.22105    -0.044400   \n",
      "2       -0.389389 -0.381598  -0.093506     -0.22105     0.086600   \n",
      "3       -0.493396 -0.333302  -0.093506     -0.22105     0.134511   \n",
      "4       -0.462579 -0.310304  -0.093506     -0.22105     0.267605   \n",
      "5       -0.281531 -0.318353  -0.093506     -0.22105     0.537019   \n",
      "6       -0.389389 -0.444842  -0.093506     -0.22105     0.508289   \n",
      "7       -0.216045  0.081812  -0.093506     -0.22105     0.463269   \n",
      "8        0.311693  0.495776  -0.093506     -0.22105     0.530905   \n",
      "9        0.323249  0.419883  -0.093506     -0.22105     0.464357   \n",
      "10       0.396439  0.450930  -0.093506     -0.22105     0.513483   \n",
      "11       0.092123  0.518774  -0.093506     -0.22105     0.539909   \n",
      "12       0.107531  0.488877  -0.093506     -0.22105     0.517964   \n",
      "13       0.080567  0.178404  -0.093506     -0.22105     0.608131   \n",
      "14       0.192278  0.352039  -0.093506     -0.22105     0.366987   \n",
      "15       0.126792  0.404934  -0.093506     -0.22105     0.318657   \n",
      "16      -0.085074  0.565920  -0.093506     -0.22105     0.195153   \n",
      "17      -0.189080  0.460129  -0.093506     -0.22105     0.177522   \n",
      "18      -0.154411  0.068013  -0.093506     -0.22105     0.069053   \n",
      "19       0.265467  0.424483  -0.093506     -0.22105    -0.034768   \n",
      "20       0.446516  0.396885  -0.093506     -0.22105    -0.093483   \n",
      "21       0.230799  0.392285  -0.093506     -0.22105    -0.131343   \n",
      "22      -0.127447  0.286494  -0.093506     -0.22105    -0.144368   \n",
      "23       0.011229  0.216351  -0.093506     -0.22105    -0.193158   \n",
      "24       0.334805  0.076063  -0.093506     -0.22105    -0.251078   \n",
      "25       0.099827  0.027767  -0.093506     -0.22105    -0.317667   \n",
      "26       0.188425  0.030067  -0.093506     -0.22105    -0.332115   \n",
      "27       0.188425  0.007069  -0.093506     -0.22105    -0.370184   \n",
      "28      -0.227601 -0.000981  -0.093506     -0.22105    -0.453609   \n",
      "29      -0.304643  0.143907  -0.093506     -0.22105    -0.516010   \n",
      "30      -0.223749  0.227850  -0.093506     -0.22105    -0.566224   \n",
      "31      -0.177524  0.278445  -0.093506     -0.22105    -0.602241   \n",
      "32      -0.289235  0.307193  -0.093506     -0.22105    -0.627830   \n",
      "33      -0.558882  0.356638  -0.093506     -0.22105    -0.654800   \n",
      "34      -0.597403  0.368137  -0.093506     -0.22105    -0.670212   \n",
      "35      -0.624368  0.372737  -0.093506     -0.22105    -0.672767   \n",
      "36      -0.782304  0.426782  -0.093506     -0.22105    -0.672767   \n",
      "37      -0.474136  0.481978  -0.093506     -0.22105    -0.672767   \n",
      "38      -0.404798  0.548672  -0.093506     -0.22105    -0.672767   \n",
      "39      -0.427910  0.571670  -0.093506     -0.22105    -0.672767   \n",
      "40      -0.670593  0.142757  -0.093506     -0.22105    -0.672767   \n",
      "41      -0.635924  0.389986  -0.093506     -0.22105    -0.672767   \n",
      "42      -0.632072  0.297993  -0.093506     -0.22105    -0.672767   \n",
      "43      -0.477988  0.212901  -0.093506     -0.22105    -0.672767   \n",
      "44      -0.466431  0.253147  -0.093506     -0.22105    -0.672767   \n",
      "45      -0.481840  0.287644  -0.093506     -0.22105    -0.672767   \n",
      "46      -0.763043  0.137007  -0.093506     -0.22105    -0.672767   \n",
      "47      -0.651332  0.335940  -0.093506     -0.22105    -0.672767   \n",
      "\n",
      "    PAR (�mol/m�/s)  max. PAR (�mol/m�/s)  Tlog (degC)        OT  \n",
      "0         -0.047046             -0.112383    -0.307310  0.024601  \n",
      "1         -0.034783             -0.068612    -0.286002  0.024080  \n",
      "2          0.096526             -0.009966    -0.272214  0.028768  \n",
      "3          0.146072              0.033230    -0.257172  0.027205  \n",
      "4          0.277210              0.203245    -0.237117  0.021475  \n",
      "5          0.538888              0.347334    -0.212048  0.021736  \n",
      "6          0.505922              0.334526    -0.186979  0.018350  \n",
      "7          0.472464              0.420855    -0.158150  0.012099  \n",
      "8          0.529381              0.452906    -0.129321  0.001942  \n",
      "9          0.459581              0.295559    -0.102998  0.001161  \n",
      "10         0.509875              0.295714    -0.087957  0.000379  \n",
      "11         0.532885              0.317118    -0.081690 -0.000662  \n",
      "12         0.512097              0.412151    -0.075423 -0.001965  \n",
      "13         0.598924              0.447994    -0.065395 -0.000142  \n",
      "14         0.358739              0.352308    -0.047847 -0.001444  \n",
      "15         0.314599              0.197448    -0.031552 -0.001444  \n",
      "16         0.193181              0.068280    -0.024031 -0.002746  \n",
      "17         0.173205              0.042525    -0.020271 -0.002746  \n",
      "18         0.068303              0.019629    -0.016510 -0.002486  \n",
      "19        -0.033245             -0.096668    -0.012750 -0.002486  \n",
      "20        -0.093665             -0.138760    -0.017764 -0.002486  \n",
      "21        -0.132528             -0.175039    -0.034059 -0.002486  \n",
      "22        -0.147120             -0.197048    -0.049100 -0.002746  \n",
      "23        -0.197178             -0.208162    -0.060381 -0.002486  \n",
      "24        -0.256573             -0.262922    -0.070409  0.000119  \n",
      "25        -0.321287             -0.313345    -0.085450  0.002463  \n",
      "26        -0.340430             -0.310423    -0.104252  0.002984  \n",
      "27        -0.376238             -0.338075    -0.123054  0.002463  \n",
      "28        -0.454990             -0.393239    -0.143109  0.004807  \n",
      "29        -0.515794             -0.447688    -0.161910  0.007672  \n",
      "30        -0.565425             -0.488117    -0.178205  0.005588  \n",
      "31        -0.602194             -0.522577    -0.195754  0.006109  \n",
      "32        -0.626486             -0.540126    -0.215809  0.005849  \n",
      "33        -0.651441             -0.558965    -0.238371  0.007672  \n",
      "34        -0.668319             -0.576001    -0.260933  0.009495  \n",
      "35        -0.675989             -0.583757    -0.283495  0.010537  \n",
      "36        -0.679493             -0.588296    -0.301043  0.010537  \n",
      "37        -0.679493             -0.588296    -0.319845  0.013402  \n",
      "38        -0.679493             -0.588296    -0.344914  0.012881  \n",
      "39        -0.679493             -0.588296    -0.374997  0.014443  \n",
      "40        -0.679493             -0.588296    -0.410093  0.023559  \n",
      "41        -0.679493             -0.588296    -0.442683  0.021996  \n",
      "42        -0.679493             -0.588296    -0.476526  0.027205  \n",
      "43        -0.679493             -0.588296    -0.510369  0.028247  \n",
      "44        -0.679493             -0.588296    -0.541705  0.024861  \n",
      "45        -0.679493             -0.588296    -0.570534  0.023819  \n",
      "46        -0.679493             -0.588296    -0.598110  0.027466  \n",
      "47        -0.679493             -0.588296    -0.623179  0.030070  \n",
      "\n",
      "[48 rows x 22 columns]\n",
      "\n",
      "\n",
      "Shape of input tensor before permutation: torch.Size([1, 48, 21])\n",
      "Shape of input tensor after permutation: torch.Size([1, 21, 48])\n",
      "RuntimeError during model prediction: The size of tensor a (6) must match the size of tensor b (13) at non-singleton dimension 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot select an axis to squeeze out which has size not equal to one",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 117\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Convert prediction to DataFrame if successful\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n\u001b[0;32m--> 117\u001b[0m     prediction_2d \u001b[38;5;241m=\u001b[39m \u001b[43mprediction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Remove the batch dimension and permute back\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# Create DataFrame for prediction\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     columns \u001b[38;5;241m=\u001b[39m weather_df\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# Exclude 'date' column\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot select an axis to squeeze out which has size not equal to one"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_provider.data_loader import Dataset_Custom\n",
    "from models.PatchTST import Model\n",
    "\n",
    "# Define model parameters to match training\n",
    "class Configs:\n",
    "    enc_in = 21\n",
    "    seq_len = 48\n",
    "    pred_len = 24\n",
    "    e_layers = 1\n",
    "    n_heads = 4\n",
    "    d_model = 32\n",
    "    d_ff = 64\n",
    "    dropout = 0.1\n",
    "    fc_dropout = 0.1\n",
    "    head_dropout = 0\n",
    "    patch_len = 4  # Match with training configuration\n",
    "    stride = 4\n",
    "    individual = False\n",
    "    revin = True\n",
    "    affine = False\n",
    "    subtract_last = False\n",
    "    decomposition = False\n",
    "    kernel_size = 25\n",
    "    padding_patch = 'end'\n",
    "    max_seq_len = 1024\n",
    "    d_k = None\n",
    "    d_v = None\n",
    "    norm = 'BatchNorm'\n",
    "    attn_dropout = 0.0\n",
    "    act = 'gelu'\n",
    "    key_padding_mask = 'auto'\n",
    "    padding_var = None\n",
    "    attn_mask = None\n",
    "    res_attention = True\n",
    "    pre_norm = False\n",
    "    store_attn = False\n",
    "    pe = 'zeros'\n",
    "    learn_pe = True\n",
    "    pretrain_head = False\n",
    "    head_type = 'flatten'\n",
    "    verbose = False\n",
    "\n",
    "configs = Configs()\n",
    "\n",
    "# Load model checkpoint\n",
    "checkpoint_path = '/home/jaleed/PatchTST/PatchTST_supervised/checkpoints/weather_quick_test_48_24_PatchTST_custom_ftM_sl48_ll48_pl24_dm32_nh4_el1_dl1_df64_fc1_ebtimeF_dtTrue_Exp_0/checkpoint.pth'\n",
    "model = Model(configs)\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(checkpoint_path)\n",
    "\n",
    "# Debugging statement: Print keys of the state_dict\n",
    "print(\"Keys in the loaded state_dict:\")\n",
    "print(state_dict.keys())\n",
    "\n",
    "# Debugging statement: Print shapes of tensors in the state_dict\n",
    "for k, v in state_dict.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "\n",
    "# Attempt to load the state dictionary into the model\n",
    "try:\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(\"Model state dictionary loaded successfully.\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"RuntimeError during model state_dict loading: {e}\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Load the test dataset\n",
    "test_data_path = '/home/jaleed/PatchTST/PatchTST_supervised/dataset/weather/weather.csv'\n",
    "weather_df = pd.read_csv(test_data_path)\n",
    "\n",
    "# Define the test dataset\n",
    "test_dataset = Dataset_Custom(root_path='/home/jaleed/PatchTST/PatchTST_supervised/dataset/weather/', data_path='weather.csv', flag='test', size=[configs.seq_len, configs.pred_len, configs.seq_len], features='M', target='OT')\n",
    "\n",
    "# Get input sequence\n",
    "input_sequences = []\n",
    "input_dates = []\n",
    "\n",
    "# Using a smaller sample size for quick testing\n",
    "num_samples = 1\n",
    "for i in range(num_samples):\n",
    "    sample = test_dataset[i]\n",
    "    input_sequences.append(sample[0])  # Directly append the sample as it is already a NumPy array\n",
    "    start_idx = weather_df.shape[0] - len(test_dataset) + i\n",
    "    input_dates.append(weather_df.iloc[start_idx:start_idx + configs.seq_len]['date'].values)\n",
    "\n",
    "input_sequences = np.stack(input_sequences)  # Shape should be (num_samples, seq_len, enc_in)\n",
    "input_tensors = torch.tensor(input_sequences).float()  # Convert to tensor\n",
    "\n",
    "# Print the input sequence\n",
    "flattened_input_sequences = input_sequences.reshape(-1, input_sequences.shape[-1])\n",
    "flattened_dates = np.array(input_dates).reshape(-1)\n",
    "df_input = pd.DataFrame(flattened_input_sequences, columns=weather_df.columns[1:])\n",
    "df_input.insert(0, 'date', flattened_dates)\n",
    "print(\"Input sequence:\")\n",
    "print(df_input)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check the shape of the input tensor\n",
    "print(f\"Shape of input tensor before permutation: {input_tensors.shape}\")\n",
    "\n",
    "# Run the model on the sequence\n",
    "with torch.no_grad():\n",
    "    input_tensors = input_tensors.permute(0, 2, 1)  # permute to [batch_size, num_features, seq_len]\n",
    "    print(f\"Shape of input tensor after permutation: {input_tensors.shape}\")\n",
    "    try:\n",
    "        prediction = model(input_tensors)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"RuntimeError during model prediction: {e}\")\n",
    "\n",
    "# Convert prediction to DataFrame if successful\n",
    "if 'prediction' in locals():\n",
    "    prediction_2d = prediction.squeeze(0).permute(1, 0).numpy()  # Remove the batch dimension and permute back\n",
    "\n",
    "    # Create DataFrame for prediction\n",
    "    columns = weather_df.columns[1:]  # Exclude 'date' column\n",
    "    df_prediction = pd.DataFrame(prediction_2d, columns=columns)\n",
    "\n",
    "    # Generate timestamps for prediction\n",
    "    last_timestamp = pd.to_datetime(flattened_dates[-1])\n",
    "    pred_timestamps = pd.date_range(last_timestamp, periods=configs.pred_len + 1, freq='10T')[1:]  # Generate next 24 timestamps with 10-minute intervals\n",
    "\n",
    "    df_prediction.insert(0, 'date', pred_timestamps)\n",
    "\n",
    "    # Print the prediction\n",
    "    print(\"Prediction sequence:\")\n",
    "    print(df_prediction)\n",
    "    print(\"\\n\")\n",
    "else:\n",
    "    print(\"Prediction failed due to RuntimeError.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b017bc94-b9cb-4d76-8dd1-9fef760d2533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

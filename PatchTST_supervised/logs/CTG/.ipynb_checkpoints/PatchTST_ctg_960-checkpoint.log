Args in experiment:
Namespace(activation='gelu', affine=0, batch_size=128, c_out=1, checkpoints='./checkpoints/ctg', d_ff=256, d_layers=1, d_model=128, data='CTG', data_path='X.npy', dec_in=2, decomposition=0, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.2, e_layers=3, embed='timeF', embed_type=0, enc_in=2, factor=1, fc_dropout=0.2, features='M', freq='h', gpu=0, head_dropout=0.0, individual=0, is_training=1, itr=1, kernel_size=25, learning_rate=0.0001, loss='cross_entropy', lradj='type3', model='PatchTST', model_id='ctg_960', moving_avg=25, n_heads=16, num_classes=2, num_workers=10, output_attention=False, padding_patch='end', patch_len=16, patience=20, pct_start=0.3, random_seed=2021, revin=1, root_path='./dataset/', seq_len=960, stride=8, subtract_last=0, target='OT', test_flop=False, train_epochs=100, use_amp=False, use_gpu=False, use_multi_gpu=False)
Use CPU
>>>>>>>start training : ctg_960_PatchTST_CTG_sl960_nc2_dm128_nh16_el3_df256_fc1_Exp>>>>>>>>>>>>>>>>>>>>>>>>>>
train 20589
val 20589
test 20589
	iters: 100, epoch: 1 | loss: 0.6926201
	speed: 1.1830s/iter; left time: 18811.5724s
Epoch: 1 cost time: 189.53744554519653
Epoch: 1, Steps: 160 | Train Loss: 0.6940736 Vali Loss: 0.6930750 Test Loss: 0.6930752
Validation loss decreased (inf --> 0.693075).  Saving model ...
Updating learning rate to 0.0001
	iters: 100, epoch: 2 | loss: 0.6931472
	speed: 3.5869s/iter; left time: 56461.4775s
Epoch: 2 cost time: 202.2313494682312
